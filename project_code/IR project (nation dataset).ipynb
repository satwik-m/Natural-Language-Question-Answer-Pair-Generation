{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'Brazil', '1': 'Burma', '2': 'China', '3': 'Cuba', '4': 'Egypt', '5': 'India', '6': 'Indonesia', '7': 'Israel', '8': 'Jordan', '9': 'Netherlands', '10': 'Poland', '11': 'USSR', '12': 'UK', '13': 'USA'}\n",
      "{'0': 'economicaid', '1': 'releconomicaid', '2': 'treaties', '3': 'reltreaties', '4': 'officialvisits', '5': 'conferences', '6': 'exportbooks', '7': 'relexportbooks', '8': 'booktranslations', '9': 'relbooktranslations', '10': 'warning', '11': 'violentactions', '12': 'militaryactions', '13': 'duration', '14': 'negativebehavior', '15': 'severdiplomatic', '16': 'expeldiplomats', '17': 'boycottembargo', '18': 'aidenemy', '19': 'negativecomm', '20': 'accusation', '21': 'protests', '22': 'unoffialacts', '23': 'attackembassy', '24': 'nonviolentbehavior', '25': 'weightedunvote', '26': 'unweightedunvote', '27': 'tourism', '28': 'reltourism', '29': 'tourism3', '30': 'emigrants', '31': 'relemigrants', '32': 'emigrants3', '33': 'students', '34': 'relstudents', '35': 'exports', '36': 'relexports', '37': 'exports3', '38': 'intergovorgs', '39': 'relintergovorgs', '40': 'ngo', '41': 'relngo', '42': 'intergovorgs3', '43': 'ngoorgs3', '44': 'embassy', '45': 'reldiplomacy', '46': 'timesincewar', '47': 'timesinceally', '48': 'lostterritory', '49': 'dependent', '50': 'independence', '51': 'commonbloc0', '52': 'blockpositionindex', '53': 'militaryalliance', '54': 'commonbloc1', '55': 'commonbloc2'}\n"
     ]
    }
   ],
   "source": [
    "entity={}\n",
    "with open(\"kg-data-master/nation/entities.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        line=line.split()\n",
    "        entity[line[0]]=line[1]\n",
    "print(entity)\n",
    "\n",
    "relation={}\n",
    "with open(\"kg-data-master/nation/relations.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        line=line.split()\n",
    "        relation[line[0]]=line[1]\n",
    "print(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node1:\n",
    "    def __init__(self,etype):\n",
    "        self.etype=etype\n",
    "        self.p=[]\n",
    "        self.bp=[]\n",
    "        \n",
    "class Node2:\n",
    "    def __init__(self,obj,pred):\n",
    "        self.obj=obj\n",
    "        self.pred=pred\n",
    "        \n",
    "class KG:\n",
    "    def __init__(self):\n",
    "        self.d={}\n",
    "        \n",
    "    def insert(self,sub,pred,obj):\n",
    "        if sub not in self.d:\n",
    "            self.d[sub]=Node1('country')\n",
    "        \n",
    "        if obj not in self.d:\n",
    "            self.d[obj]=Node1('country')\n",
    "            \n",
    "        self.d[sub].p.append(Node2(obj,pred))\n",
    "        self.d[obj].bp.append(Node2(sub,pred))\n",
    "        \n",
    "K=KG()\n",
    "with open(\"kg-data-master/nation/triples.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        line=line.split()\n",
    "        K.insert(entity[line[1]],relation[line[0]],entity[line[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the entity to form QA pairs: China\n",
      "economicaid\n",
      "['China', 'country', 'economicaid', 'Indonesia', 'country']\n"
     ]
    }
   ],
   "source": [
    "entity=input('enter the entity to form QA pairs: ')\n",
    "objects=K.d[entity].p\n",
    "print(objects[0].pred)\n",
    "tuples1=[]\n",
    "for i in range(len(objects)):\n",
    "    tuples1.append([entity,K.d[entity].etype,objects[i].pred,objects[i].obj,K.d[objects[i].obj].etype])\n",
    "print(tuples1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Forward Relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['China', 'country', 'reltreaties', 'Indonesia', 'country'], ['China', 'country', 'boycottembargo', 'USSR', 'country'], ['China', 'country', 'aidenemy', 'USA', 'country'], ['China', 'country', 'tourism', 'USSR', 'country'], ['China', 'country', 'reltourism', 'USSR', 'country'], ['China', 'country', 'emigrants', 'USA', 'country'], ['China', 'country', 'militaryalliance', 'Cuba', 'country']]\n"
     ]
    }
   ],
   "source": [
    "di={}\n",
    "for i in tuples1:\n",
    "    if i[2] not in di:\n",
    "        di[i[2]]=1\n",
    "    else:\n",
    "        di[i[2]]+=1\n",
    "\n",
    "unique=[]\n",
    "for i in tuples1:\n",
    "    if di[i[2]]==1:\n",
    "        unique.append(i)\n",
    "print(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords=[]\n",
    "answers=[]\n",
    "for i in unique:\n",
    "    keywords.append(' '.join([i[0],i[2],i[4]]))\n",
    "    answers.append(i[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Reverse Relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'treaties'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects=K.d[entity].bp\n",
    "objects[0].obj\n",
    "objects[0].pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treaties\n",
      "['Cuba', 'country', 'treaties', 'China', 'country']\n"
     ]
    }
   ],
   "source": [
    "objects=K.d[entity].bp\n",
    "print(objects[0].pred)\n",
    "tuples2=[]\n",
    "for i in range(len(objects)):\n",
    "    tuples2.append([objects[i].obj,K.d[objects[i].obj].etype,objects[i].pred,entity,K.d[entity].etype])\n",
    "print(tuples2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['UK', 'country', 'relexportbooks', 'China', 'country'], ['India', 'country', 'protests', 'China', 'country'], ['Indonesia', 'country', 'unoffialacts', 'China', 'country'], ['Indonesia', 'country', 'students', 'China', 'country']]\n"
     ]
    }
   ],
   "source": [
    "di={}\n",
    "for i in tuples2:\n",
    "    if i[2] not in di:\n",
    "        di[i[2]]=1\n",
    "    else:\n",
    "        di[i[2]]+=1\n",
    "\n",
    "unique=[]\n",
    "for i in tuples2:\n",
    "    if di[i[2]]==1:\n",
    "        unique.append(i)\n",
    "print(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in unique:\n",
    "    keywords.append(' '.join([i[0],i[2],i[4]]))\n",
    "    answers.append(i[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "keywords=np.array(keywords,dtype='U74')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=2504, size=5, alpha=0.025)\n",
      "[ 0.09185895 -0.04627458  0.03967929  0.00535333 -0.07861977]\n",
      "Word2Vec(vocab=2504, size=5, alpha=0.025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for i in range(len(sentence)):\\n    print(sentence[i])\\n    print(token_words[i])'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from gensim.models import Word2Vec\n",
    "from array import array\n",
    "from numpy import *\n",
    "stop_words = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "\n",
    "txt = \"what movies did sally field win an oscar for? \\nwho played mulder in the x files? what is the name of dawn french's first novel? what song is monifah famous for? what illness does michael j fox have? what are some of the awards taylor swift has won? who is ben roethlisberger parents? what is modern egyptian language? who plays the voice of lois griffin? who did lenny kravitz marry? what money system does australia use? what to do in roatan bay? who plays jacob black in the twilight movies? what to see outside of paris? who does christopher walken play in batman returns? what contribution did maurice wilkins make to dna? what is there to do around austin texas? what team did deion sanders play for in baseball? what county is plainfield il in? who plays young john winchester in supernatural? what airport fly into miami? where is napoleon buried? what language do they speak in spain wikipedia? what is the name of the pittsburgh steelers stadium? where did matthias schleiden go to school? where is the great pyramid of giza situated? what beach did the canadians assault? what do people from spain speak? where was stephen g. breyer born? what tv show did joey lawrence play on? what kind of language do they speak in greece? what did corey haim really die of? where did pavlova originate? who does nikki reed play in the movie twilight? what events does stephanie rice compete in? what highschool did bill gates graduated from? who did the voice of kitt in knight rider? what airport do you fly into to get to cabo san lucas? who is the coach of inter milan now? where is bob marley grave? what wild animals live in colorado? which city held the summer olympics twice? what currency does brazil use? what government did japan have? what language does people in netherlands speak? what type of government is the us government? what movies did jj abrams direct? what did ronnie radke do? what club does ronaldinho play for 2012? what did jordyn wieber win gold for? who did john fox coach for? what is there to see near the grand canyon? what teams did manny ramirez play for? what is the name of money in brazil? who played as barney? what should you see in london? \"\n",
    "#txt=\"Hello How are you?\"\n",
    "# sent_tokenize is one of instances of \n",
    "# PunktSentenceTokenizer from the nltk.tokenize.punkt module \n",
    "i=0\n",
    "with open(\"untitled.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        txt=txt+line[:-1]+\" \"\n",
    "    i+=1\n",
    "sentence=[]\n",
    "token_words=[]\n",
    "tokenized = sent_tokenize(txt) \n",
    "    \n",
    "doc=[]\n",
    "for i in tokenized: \n",
    "    token_words.append(i)\n",
    "    # Word tokenizers is used to find the words \n",
    "    # and punctuation in a string \n",
    "    wordsList = nltk.word_tokenize(i) \n",
    "\n",
    "    # removing stop words from wordList \n",
    "    wordsList = [w for w in wordsList if not w in stop_words] \n",
    "\n",
    "    # Using a Tagger. Which is part-of-speech \n",
    "    # tagger or POS-tagger. \n",
    "    tagged = nltk.pos_tag(wordsList) \n",
    "    pos_list=[\"JJ\",\"JJR\",\"JJS\",\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"VB\",\"VBG\",\"VBD\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "    word_set=[]\n",
    "    for i in tagged:\n",
    "        if (i[1] in pos_list):\n",
    "            word_set.append(i[0])\n",
    "    sentence.append(word_set)\n",
    "    doc.append(word_set)\n",
    "#print(doc)\n",
    "# train model\n",
    "model = Word2Vec(doc, min_count=1,size=5)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "#print(words)\n",
    "# access vector for one word\n",
    "print(model['movies'])\n",
    "\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)\n",
    "\n",
    "your_word_vector = array([0.00482347, -0.09845231,  0.01818257, -0.03367689,  0.0329078], dtype=float32)\n",
    "model.wv.most_similar(positive=[ your_word_vector], topn=1)\n",
    "'''for i in range(len(sentence)):\n",
    "    print(sentence[i])\n",
    "    print(token_words[i])'''\n",
    "#print(len(token_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset.txt\",\"w\") as f:\n",
    "    for i in range(len(sentence)):\n",
    "        f.write(token_words[i]+'\\t'+\" \".join(sentence[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: k2q.pkl\n",
      "[what movies did sally field win an oscar for] => [movies field win oscar]\n",
      "[who played mulder in the x files] => [played mulder x files]\n",
      "[what is the name of dawn frenchs first novel] => [name dawn french first novel]\n",
      "[what song is monifah famous for] => [song famous]\n",
      "[what illness does michael j fox have] => [illness michael j fox]\n",
      "[what are some of the awards taylor swift has won] => [awards taylor swift]\n",
      "[who is ben roethlisberger parents] => [ben roethlisberger parents]\n",
      "[what is modern egyptian language] => [modern egyptian language]\n",
      "[who plays the voice of lois griffin] => [plays voice lois griffin]\n",
      "[who did lenny kravitz marry] => [lenny kravitz marry]\n",
      "[what money system does australia use] => [money system australia use]\n",
      "[what to do in roatan bay] => [roatan bay]\n",
      "[who plays jacob black in the twilight movies] => [plays jacob black twilight movies]\n",
      "[what to see outside of paris] => [see outside paris]\n",
      "[who does christopher walken play in batman returns] => [christopher play batman returns]\n",
      "[what contribution did maurice wilkins make to dna] => [contribution maurice wilkins make dna]\n",
      "[what is there to do around austin texas] => [austin texas]\n",
      "[what team did deion sanders play for in baseball] => [team deion sanders play baseball]\n",
      "[what county is plainfield il in] => [county plainfield il]\n",
      "[who plays young john winchester in supernatural] => [plays young john winchester supernatural]\n",
      "[what airport fly into miami] => [airport fly miami]\n",
      "[where is napoleon buried] => [napoleon buried]\n",
      "[what language do they speak in spain wikipedia] => [language speak spain wikipedia]\n",
      "[what is the name of the pittsburgh steelers stadium] => [name pittsburgh steelers stadium]\n",
      "[where did matthias schleiden go to school] => [matthias schleiden go school]\n",
      "[where is the great pyramid of giza situated] => [great pyramid giza situated]\n",
      "[what beach did the canadians assault] => [beach canadians assault]\n",
      "[what do people from spain speak] => [people spain speak]\n",
      "[where was stephen g breyer born] => [stephen g breyer born]\n",
      "[what tv show did joey lawrence play on] => [tv show joey lawrence play]\n",
      "[what kind of language do they speak in greece] => [kind language speak greece]\n",
      "[what did corey haim really die of] => [corey haim die]\n",
      "[where did pavlova originate] => [pavlova originate]\n",
      "[who does nikki reed play in the movie twilight] => [nikki reed play movie twilight]\n",
      "[what events does stephanie rice compete in] => [events stephanie rice compete]\n",
      "[what highschool did bill gates graduated from] => [highschool bill gates graduated]\n",
      "[who did the voice of kitt in knight rider] => [voice kitt knight rider]\n",
      "[what airport do you fly into to get to cabo san lucas] => [airport fly get cabo san lucas]\n",
      "[who is the coach of inter milan now] => [coach inter milan]\n",
      "[where is bob marley grave] => [bob marley grave]\n",
      "[what wild animals live in colorado] => [wild animals live colorado]\n",
      "[which city held the summer olympics twice] => [city held summer olympics]\n",
      "[what currency does brazil use] => [currency brazil use]\n",
      "[what government did japan have] => [government japan]\n",
      "[what language does people in netherlands speak] => [language people netherlands speak]\n",
      "[what type of government is the us government] => [type government government]\n",
      "[what movies did jj abrams direct] => [movies jj abrams direct]\n",
      "[what did ronnie radke do] => [ronnie radke]\n",
      "[what club does ronaldinho play for] => [club ronaldinho play]\n",
      "[what did jordyn wieber win gold for] => [jordyn win gold]\n",
      "[who did john fox coach for] => [john fox coach]\n",
      "[what is there to see near the grand canyon] => [see grand canyon]\n",
      "[what teams did manny ramirez play for] => [teams manny ramirez play]\n",
      "[what is the name of money in brazil] => [name money brazil]\n",
      "[who played as barney] => [played barney]\n",
      "[what should you see in london] => [see london]\n",
      "[where robert frost went to school] => [robert frost went school]\n",
      "[what films has kristen stewart starred in] => [films kristen starred]\n",
      "[what movies johnny depp is in] => [movies johnny depp]\n",
      "[who did russians descend from] => [russians descend]\n",
      "[what musical instruments did duke ellington play] => [musical instruments duke ellington play]\n",
      "[what position did george washington serve in the constitutional convention] => [position george washington serve constitutional convention]\n",
      "[what kinda music does john mayer sing] => [kinda music john mayer sing]\n",
      "[what state is george washington university located in] => [state george washington university located]\n",
      "[who did bynum get traded to] => [bynum get traded]\n",
      "[what is the current time in nigeria lagos] => [current time nigeria lagos]\n",
      "[who was the colts coach in] => [colts coach]\n",
      "[who did ayrton senna drive for] => [ayrton senna drive]\n",
      "[who was the general for the british in the revolutionary war] => [general british revolutionary war]\n",
      "[who is my state senator and representative minnesota] => [state senator representative minnesota]\n",
      "[what places made up the western roman empire] => [places made western roman empire]\n",
      "[what is the official language of china] => [official language china]\n",
      "[what college did steve jobs attend] => [college steve jobs attend]\n",
      "[where did henry hudson come from] => [henry hudson come]\n",
      "[what did the harlem renaissance created] => [harlem renaissance created]\n",
      "[what continent does the amazon river flow through] => [continent amazon river flow]\n",
      "[what language do irish people speak] => [language irish people speak]\n",
      "[what has ian somerhalder played in] => [ian somerhalder played]\n",
      "[who is the prime minister of ethiopia now] => [prime minister ethiopia]\n",
      "[what type of voting system does the uk have] => [type voting system uk]\n",
      "[who was the first russian president] => [russian president]\n",
      "[who played elaine on doc martin] => [played elaine doc martin]\n",
      "[what is the language called in russia] => [language called russia]\n",
      "[what songs did mozart write] => [songs mozart write]\n",
      "[who is robert downey jr wife] => [robert downey jr wife]\n",
      "[who is angelina jolie husband name] => [jolie husband name]\n",
      "[when last did real madrid win the champions league] => [last real madrid win champions league]\n",
      "[what the currency in argentina] => [currency argentina]\n",
      "[who killed harvey milk] => [killed harvey milk]\n",
      "[who won the super bowl xliv] => [super bowl xliv]\n",
      "[where is jay cutler now] => [jay cutler]\n",
      "[what year was the first miss america pageant] => [year miss america pageant]\n",
      "[who plays stewie griffin on family guy] => [plays stewie griffin family guy]\n",
      "[what high school did president bill clinton attend] => [high school president bill clinton attend]\n",
      "[what is the postcode for london] => [postcode london]\n",
      "[what happened to the battle of shiloh] => [happened battle shiloh]\n",
      "[what time zone is london in right now] => [time london right]\n",
      "[where great britain located] => [great britain located]\n",
      "[what country was george washington from] => [country george washington]\n",
      "[what to do downtown san francisco] => [downtown san francisco]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    " \n",
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)\n",
    " \n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "    # load dataset\n",
    "filename = 'dataset.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-german pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'k2q.pkl')\n",
    "# spot check\n",
    "for i in range(100):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: k2q-both.pkl\n",
      "Saved: k2q-train.pkl\n",
      "Saved: k2q-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    " \n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "raw_dataset = load_clean_sentences('k2q.pkl')\n",
    " \n",
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:2100], dataset[2100:]\n",
    "# save\n",
    "save_clean_data(dataset, 'k2q-both.pkl')\n",
    "save_clean_data(train, 'k2q-train.pkl')\n",
    "save_clean_data(test, 'k2q-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "voc=np.array(list(dataset[:,1])+list(keywords),dtype='U74')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Vocabulary Size: 2690\n",
      "Question Max Length : 13\n",
      "Keyword Vocabulary Size: 2496\n",
      "Keyword Max Length: 7\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 7, 256)            638976    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 13, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 13, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 13, 2690)          691330    \n",
      "=================================================================\n",
      "Total params: 2,380,930\n",
      "Trainable params: 2,380,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2100 samples, validate on 135 samples\n",
      "Epoch 1/20\n",
      " - 27s - loss: 5.6315 - val_loss: 4.2036\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.20359, saving model to modelcopy2.h5\n",
      "Epoch 2/20\n",
      " - 17s - loss: 3.9087 - val_loss: 3.9373\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.20359 to 3.93728, saving model to modelcopy2.h5\n",
      "Epoch 3/20\n",
      " - 17s - loss: 3.5681 - val_loss: 3.5510\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.93728 to 3.55104, saving model to modelcopy2.h5\n",
      "Epoch 4/20\n",
      " - 19s - loss: 3.1951 - val_loss: 3.2479\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.55104 to 3.24790, saving model to modelcopy2.h5\n",
      "Epoch 5/20\n",
      " - 18s - loss: 2.9844 - val_loss: 3.1612\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.24790 to 3.16119, saving model to modelcopy2.h5\n",
      "Epoch 6/20\n",
      " - 18s - loss: 2.8767 - val_loss: 3.0802\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.16119 to 3.08019, saving model to modelcopy2.h5\n",
      "Epoch 7/20\n",
      " - 18s - loss: 2.8132 - val_loss: 3.0537\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.08019 to 3.05373, saving model to modelcopy2.h5\n",
      "Epoch 8/20\n",
      " - 18s - loss: 2.7662 - val_loss: 3.0401\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.05373 to 3.04009, saving model to modelcopy2.h5\n",
      "Epoch 9/20\n",
      " - 18s - loss: 2.7352 - val_loss: 3.0405\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.04009\n",
      "Epoch 10/20\n",
      " - 18s - loss: 2.6989 - val_loss: 3.0241\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.04009 to 3.02411, saving model to modelcopy2.h5\n",
      "Epoch 11/20\n",
      " - 17s - loss: 2.6608 - val_loss: 3.0149\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.02411 to 3.01494, saving model to modelcopy2.h5\n",
      "Epoch 12/20\n",
      " - 17s - loss: 2.6309 - val_loss: 3.0345\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 3.01494\n",
      "Epoch 13/20\n",
      " - 17s - loss: 2.6009 - val_loss: 3.0290\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 3.01494\n",
      "Epoch 14/20\n",
      " - 18s - loss: 2.5715 - val_loss: 3.0323\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 3.01494\n",
      "Epoch 15/20\n",
      " - 18s - loss: 2.5453 - val_loss: 3.0316\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 3.01494\n",
      "Epoch 16/20\n",
      " - 17s - loss: 2.5207 - val_loss: 3.0516\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.01494\n",
      "Epoch 17/20\n",
      " - 19s - loss: 2.4991 - val_loss: 3.0499\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 3.01494\n",
      "Epoch 18/20\n",
      " - 18s - loss: 2.4771 - val_loss: 3.0507\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3.01494\n",
      "Epoch 19/20\n",
      " - 17s - loss: 2.4558 - val_loss: 3.0605\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3.01494\n",
      "Epoch 20/20\n",
      " - 18s - loss: 2.4315 - val_loss: 3.0486\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.01494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c960c0bef0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    " \n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    " \n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    " \n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    " \n",
    "# define RNN model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    " \n",
    "# load datasets\n",
    "dataset = load_clean_sentences('k2q-both.pkl')\n",
    "train = load_clean_sentences('k2q-train.pkl')\n",
    "test = load_clean_sentences('k2q-test.pkl')\n",
    " \n",
    "# prepare question tokenizer\n",
    "q_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "q_vocab_size = len(q_tokenizer.word_index) + 1\n",
    "q_length = max_length(dataset[:, 0])\n",
    "print('Question Vocabulary Size: %d' % q_vocab_size)\n",
    "print('Question Max Length : %d' % (q_length))\n",
    "# prepare keyword tokenizer\n",
    "k_tokenizer = create_tokenizer(voc)\n",
    "k_vocab_size = len(k_tokenizer.word_index) + 1\n",
    "k_length = max_length(voc)\n",
    "print('Keyword Vocabulary Size: %d' % k_vocab_size)\n",
    "print('Keyword Max Length: %d' % (k_length))\n",
    " \n",
    "# prepare training data\n",
    "trainX = encode_sequences(k_tokenizer, k_length, train[:, 1])\n",
    "trainY = encode_sequences(q_tokenizer, q_length, train[:, 0])\n",
    "z=trainY[:]\n",
    "\n",
    "trainY = encode_output(trainY, q_vocab_size)\n",
    "\n",
    "testX = encode_sequences(k_tokenizer, k_length, test[:, 1])\n",
    "testY = encode_sequences(q_tokenizer, q_length, test[:, 0])\n",
    "\n",
    "testY = encode_output(testY, q_vocab_size)\n",
    "\n",
    " \n",
    "# define model\n",
    "model = define_model(k_vocab_size, q_vocab_size, k_length, q_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "filename = 'modelcopy2.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=20, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "voc=np.array(list(dataset[:,1])+list(keywords),dtype='U74')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:\n",
      "src=[russian president], target=[who was the first russian president], predicted=[what is the russian president]\n",
      "src=[garrett little liars], target=[who is garrett pretty little liars], predicted=[what is the garrett little liars]\n",
      "src=[miami dolphins win super bowl], target=[when did miami dolphins win super bowl], predicted=[what is the miami dolphins win super bowl]\n",
      "src=[position mitt romney hold], target=[what position does mitt romney hold], predicted=[what is the position mitt romney hold]\n",
      "src=[dan patrick studio], target=[where is the dan patrick studio], predicted=[what is the dan patrick studio]\n",
      "src=[prime minister new zealand], target=[who is the prime minister of new zealand now], predicted=[what is the prime minister new zealand]\n",
      "src=[type government iraq], target=[what type of government does iraq have], predicted=[what is the type government iraq]\n",
      "src=[last time chicago bulls championship], target=[when is the last time the chicago bulls won a championship], predicted=[what is the last time chicago bulls championship]\n",
      "src=[currency republic congo], target=[what is the currency in the republic of congo], predicted=[what is the currency republic congo]\n",
      "src=[original voice meg family guy], target=[who was the original voice of meg on family guy], predicted=[what is the original voice meg family guy]\n",
      "src=[country osama bin laden live], target=[what country did osama bin laden live in], predicted=[what is the country osama bin laden live]\n",
      "src=[controls panama], target=[who controls panama], predicted=[what is the controls panama]\n",
      "src=[jackie kennedy go college], target=[where did jackie kennedy go to college], predicted=[what is the jackie kennedy go college]\n",
      "src=[benjamin franklin inventions], target=[what was one of benjamin franklins inventions], predicted=[what is the benjamin franklin inventions]\n",
      "src=[money used ukraine], target=[what money is used in ukraine], predicted=[what is the money used ukraine]\n",
      "src=[instrument mike huckabee play], target=[what instrument does mike huckabee play], predicted=[what is the instrument mike huckabee play]\n",
      "src=[founded new democratic party], target=[who founded the new democratic party], predicted=[what is the founded new democratic party]\n",
      "src=[cuban missile crisis happen], target=[where did the cuban missile crisis happen], predicted=[what is the cuban missile crisis happen]\n",
      "src=[robert boyle study], target=[where did robert boyle study], predicted=[what is the robert boyle study]\n",
      "src=[language jewish speak], target=[what language do jewish speak], predicted=[what is the language jewish speak]\n",
      "src=[government spain today], target=[what is the government of spain today], predicted=[what is the government spain today]\n",
      "src=[city purdue university located], target=[what city is purdue university located], predicted=[what is the city purdue university located]\n",
      "src=[kind cancer bob marley died], target=[what kind of cancer did bob marley died from], predicted=[what is the kind cancer bob marley died]\n",
      "src=[head coach new orleans saints], target=[who is the head coach of the new orleans saints], predicted=[what is the head coach new orleans saints]\n",
      "src=[lbj die], target=[what did lbj die of], predicted=[what is the lbj die]\n",
      "src=[large mountain range western united states], target=[what is the large mountain range in the western united states], predicted=[what is the large mountain range western united states]\n",
      "src=[name money brazil], target=[what is the name of money in brazil], predicted=[what is the name money brazil]\n",
      "src=[sports people france play], target=[what sports do people in france play], predicted=[what is the sports people france play]\n",
      "src=[mcgee start ncis], target=[when did mcgee start on ncis], predicted=[what is the mcgee start ncis]\n",
      "src=[time new orleans], target=[what is time zone in new orleans], predicted=[what is the time new orleans]\n",
      "src=[leach coach], target=[where does mike leach coach], predicted=[what is the leach coach]\n",
      "src=[chef chuck hughes go school], target=[where did chef chuck hughes go to school], predicted=[what is the chef chuck hughes go school]\n",
      "src=[kind legal system australia], target=[what kind of legal system does australia have], predicted=[what is the kind legal system australia]\n",
      "src=[laughlin nevada], target=[what to do in laughlin nevada], predicted=[what is is laughlin nevada]\n",
      "src=[kind music john mayer play], target=[what kind of music does john mayer play], predicted=[what is the kind music john mayer play]\n",
      "src=[st louis rams play football], target=[where does the st louis rams play football], predicted=[what is the st louis rams play football]\n",
      "src=[started federal reserve board], target=[who started the federal reserve board], predicted=[what is the started federal reserve board]\n",
      "src=[language new speak], target=[what language does the new zealand speak], predicted=[what is the language new speak]\n",
      "src=[team hank baskett play season], target=[what team does hank baskett play for season], predicted=[what is the team hank baskett play season]\n",
      "src=[movies queen latifah played], target=[what movies have queen latifah played in], predicted=[what is the movies queen latifah played]\n",
      "src=[major exports usa], target=[what are major exports of the usa], predicted=[what is the major exports usa]\n",
      "src=[newt gingrich attend college], target=[where did newt gingrich attend college], predicted=[what is the newt gingrich attend college]\n",
      "src=[movies adam sandler play], target=[what movies did adam sandler play in], predicted=[what is the movies adam sandler play]\n",
      "src=[religion people pakistan], target=[what religion are most people in pakistan], predicted=[what is the religion people pakistan]\n",
      "src=[marco rubio go college], target=[where did marco rubio go to college], predicted=[what is the marco rubio go college]\n",
      "src=[form government canada], target=[what form of government does canada have], predicted=[what is the form government canada]\n",
      "src=[lanzarote world map], target=[where is lanzarote in a world map], predicted=[what is the lanzarote world map]\n",
      "src=[marilyn monroe known], target=[what was marilyn monroe known for], predicted=[what is the marilyn monroe known]\n",
      "src=[happened invasion normandy], target=[what happened after the invasion of normandy], predicted=[what is the happened invasion normandy]\n",
      "src=[language argentina], target=[what is language in argentina], predicted=[what is the language argentina]\n",
      "src=[language native americans speak], target=[what language do most native americans speak], predicted=[what is the language native americans speak]\n",
      "src=[nick grimshaw], target=[who is nick grimshaw], predicted=[what is the nick grimshaw]\n",
      "src=[kind government vietnam], target=[what kind of government does vietnam have], predicted=[what is the kind government vietnam]\n",
      "src=[year michael jordan get drafted], target=[what year did michael jordan get drafted], predicted=[what is the year michael jordan get drafted]\n",
      "src=[name alabama state tree], target=[what is the name of the alabama state tree], predicted=[what is the name alabama state tree]\n",
      "src=[language australian people speak], target=[what language do australian people speak], predicted=[what is the language australian people speak]\n",
      "src=[asiana airlines fly], target=[where does asiana airlines fly to], predicted=[what is the asiana airlines fly]\n",
      "src=[founder google go college], target=[where did the founder of google go to college], predicted=[what is the founder google go college]\n",
      "src=[joe mcelderry x factor], target=[when did joe mcelderry won x factor], predicted=[what is the joe mcelderry x factor]\n",
      "src=[france surrender], target=[where did france surrender], predicted=[what is the france surrender]\n",
      "src=[countries use english national language], target=[what countries use english as national language], predicted=[what is the countries use english national language]\n",
      "src=[school sergio romo go], target=[what school did sergio romo go to], predicted=[what is the school sergio romo go]\n",
      "src=[selena gomez born], target=[where was selena gomez really born], predicted=[what is the selena gomez born]\n",
      "src=[political party john bell], target=[what political party did john bell belong to], predicted=[what is the political party john bell]\n",
      "src=[ann romney], target=[where is ann romney from], predicted=[what who is ann romney]\n",
      "src=[see sedona arizona], target=[what to see near sedona arizona], predicted=[what is the see sedona arizona]\n",
      "src=[current president dominican republic], target=[who is the current president of the dominican republic in], predicted=[what is the current president dominican republic]\n",
      "src=[currency dominican republic bring], target=[what currency in dominican republic should i bring], predicted=[what is the currency dominican republic bring]\n",
      "src=[john lennon standing shot], target=[where was john lennon standing when he was shot], predicted=[what is the john lennon standing shot]\n",
      "src=[petr lith mean stone], target=[both petr and lith mean stone], predicted=[what is the petr lith mean stone]\n",
      "src=[current president chile], target=[who is the current president of chile], predicted=[what is the current president chile]\n",
      "src=[country japan export], target=[what country does japan export to], predicted=[what is the country japan export]\n",
      "src=[many languages philippines], target=[how many languages are there in the philippines], predicted=[what is the many languages philippines]\n",
      "src=[animal western australian flag], target=[what animal is on the western australian flag], predicted=[what is the animal western australian flag]\n",
      "src=[movies chris farley], target=[what movies did chris farley do], predicted=[what is the movies chris farley]\n",
      "src=[george bush becoming president], target=[what did george w bush do before becoming president], predicted=[what is the george bush becoming president]\n",
      "src=[runs cornelia marie], target=[who runs the cornelia marie now], predicted=[what is the runs cornelia marie]\n",
      "src=[official language spoken mexico], target=[what is the official language spoken in mexico], predicted=[what is the official language spoken mexico]\n",
      "src=[war george washington help win], target=[what war did george washington help win], predicted=[what is the war george washington help win]\n",
      "src=[kate spade], target=[what is kate spade], predicted=[what is the kate spade]\n",
      "src=[happened simone], target=[what happened to nina simone], predicted=[what is the happened simone]\n",
      "src=[plays faramir lord rings], target=[who plays faramir in lord of the rings], predicted=[what is the plays faramir lord rings]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src=[school burne hogarth establish], target=[what school did burne hogarth establish], predicted=[what is the school burne hogarth establish]\n",
      "src=[nationality andy williams], target=[what nationality was andy williams], predicted=[what is the nationality andy williams]\n",
      "src=[new orleans hornets new name], target=[what is the new orleans hornets new name], predicted=[what is the new orleans hornets new name]\n",
      "src=[timezone denver co], target=[what timezone is denver co], predicted=[what is the timezone denver co]\n",
      "src=[teams left nfl], target=[what teams are left in the nfl], predicted=[what is the teams left nfl]\n",
      "src=[type government fiji], target=[what type of government does fiji have], predicted=[what is the type government fiji]\n",
      "src=[bella name baby], target=[what bella name her baby], predicted=[what is the bella name baby]\n",
      "src=[book john steinbeck wrote people dust bowl], target=[what book did john steinbeck wrote about the people in the dust bowl], predicted=[what is the book john steinbeck wrote people dust bowl]\n",
      "src=[roshon fegan heritage], target=[what is roshon fegans heritage], predicted=[what is the roshon fegan heritage]\n",
      "src=[loma prieta earthquake occur], target=[where did the loma prieta earthquake occur], predicted=[what is the loma prieta earthquake occur]\n",
      "src=[county morristown tn], target=[what county is morristown tn in], predicted=[what is the county morristown tn]\n",
      "src=[jackie robinson go school], target=[where did jackie robinson go to school], predicted=[what is the jackie robinson go school]\n",
      "src=[city belgium], target=[what is a city in belgium], predicted=[what is the city belgium]\n",
      "src=[plays riley finn buffy vampire slayer], target=[who plays riley finn on buffy the vampire slayer], predicted=[what is the plays riley finn buffy vampire slayer]\n",
      "src=[john fox coach], target=[who did john fox coach for], predicted=[what is the john fox coach]\n",
      "src=[plays young joe dirt], target=[who plays young joe dirt], predicted=[what is the plays young joe dirt]\n",
      "src=[years orioles play world series], target=[what years did the orioles play in the world series], predicted=[what is the years orioles play world series]\n",
      "src=[johnny crawford sing], target=[what did johnny crawford sing], predicted=[what is the johnny crawford sing]\n",
      "src=[currency used switzerland], target=[what currency is used in switzerland], predicted=[what is the currency used switzerland]\n",
      "src=[ben roethlisberger parents], target=[who is ben roethlisberger parents], predicted=[what is the ben roethlisberger parents]\n",
      "src=[party lincoln], target=[which party was lincoln], predicted=[what is the party lincoln]\n",
      "src=[played young sam winchester], target=[who played young sam winchester], predicted=[what is the played young sam winchester]\n",
      "src=[county orlando fl], target=[what county is orlando fl in], predicted=[what is the county orlando fl]\n",
      "src=[airport fly get cabo san lucas], target=[what airport do you fly into to get to cabo san lucas], predicted=[what is the airport fly get cabo san lucas]\n",
      "src=[countries united kingdom], target=[what four countries are in the united kingdom], predicted=[what is the countries united kingdom]\n",
      "src=[current leader japan], target=[what is the current leader of japan], predicted=[what is the current leader japan]\n",
      "src=[names michael jackson children], target=[what are the names of michael jackson children], predicted=[what is the names michael jackson children]\n",
      "src=[played mickey days lives], target=[who played mickey on days of our lives], predicted=[what is the played mickey days lives]\n",
      "src=[movies kenya produced], target=[what movies have kenya moore produced], predicted=[what is the movies kenya produced]\n",
      "src=[dickinson die], target=[what did emily dickinson die of], predicted=[what is the dickinson die]\n",
      "src=[currency argentina], target=[what the currency in argentina], predicted=[what is the currency argentina]\n",
      "src=[language people speak iceland], target=[what language do people speak in iceland], predicted=[what is the language people speak iceland]\n",
      "src=[county charlotte north carolina], target=[what county is charlotte north carolina], predicted=[what is the county charlotte north carolina]\n",
      "src=[plays voice lois griffin], target=[who plays the voice of lois griffin], predicted=[what is the plays voice lois griffin]\n",
      "src=[great pyramid giza located], target=[where are the great pyramid of giza located], predicted=[what is the great pyramid giza located]\n",
      "src=[airport best fly milan], target=[which airport is best to fly into milan], predicted=[what is the airport best fly milan]\n",
      "src=[voice stewie griffin], target=[who has the voice of stewie griffin], predicted=[what is the voice stewie griffin]\n",
      "src=[last time oakland raiders super bowl], target=[when was the last time the oakland raiders were in the super bowl], predicted=[what is the last time oakland raiders super bowl]\n",
      "src=[american pie song], target=[what is american pie about song], predicted=[what is the american pie song]\n",
      "src=[michael jackson brothers sisters], target=[who were michael jacksons brothers and sisters], predicted=[what is the michael jackson brothers sisters]\n",
      "src=[school martin king jr attend], target=[what school did martin luther king jr attend], predicted=[what is the school martin king jr attend]\n",
      "src=[fun things seattle wa], target=[what are fun things to do in seattle wa], predicted=[what is the fun things seattle wa]\n",
      "src=[office boehner hold], target=[what office does boehner hold], predicted=[what is the office boehner hold]\n",
      "src=[happened farrah baby daddy], target=[what happened to farrah and her baby daddy], predicted=[what is the happened farrah baby daddy]\n",
      "src=[dr martin luther king jr go school], target=[where did dr martin luther king jr go to school], predicted=[what is the dr martin luther king jr go school]\n",
      "src=[major religions poland], target=[what are the major religions in poland], predicted=[what is the major religions poland]\n",
      "src=[countries caribbean], target=[what countries are in the caribbean], predicted=[what is the countries caribbean]\n",
      "src=[team magic johnson play], target=[what team did magic johnson play for], predicted=[what is the team magic johnson play]\n",
      "src=[voice kitt knight rider], target=[who does the voice of kitt in knight rider], predicted=[what is the voice kitt knight rider]\n",
      "src=[miller influenced], target=[who was arthur miller influenced by], predicted=[what is the miller influenced]\n",
      "src=[club team ronaldinho play], target=[what club team does ronaldinho play for], predicted=[what is the club team ronaldinho play]\n",
      "src=[movies angelina jolie star], target=[what movies did angelina jolie star in], predicted=[what is the movies angelina jolie star]\n",
      "src=[date john adams elected president], target=[what date was john adams elected president], predicted=[what is the date john adams elected president]\n",
      "\n",
      "Question Answer pairs:\n",
      "predicted question=[what is the China reltreaties country?]  answer=[Indonesia]\n",
      "predicted question=[what is the China boycottembargo country?]  answer=[USSR]\n",
      "predicted question=[what is the China aidenemy country?]  answer=[USA]\n",
      "predicted question=[what is the China tourism country?]  answer=[USSR]\n",
      "predicted question=[what is the China reltourism country?]  answer=[USSR]\n",
      "predicted question=[what is the China emigrants country?]  answer=[USA]\n",
      "predicted question=[what is the China militaryalliance country?]  answer=[Cuba]\n",
      "predicted question=[what is the UK relexportbooks country?]  answer=[China]\n",
      "predicted question=[what is the India protests country?]  answer=[China]\n",
      "predicted question=[what is the Indonesia unoffialacts country?]  answer=[China]\n",
      "predicted question=[what is the Indonesia students country?]  answer=[China]\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    " \n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    " \n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    " \n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    " \n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    k=list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        question = predict_sequence(model, tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        question = question.split()\n",
    "        question = ' '.join(question[:3])+' '+raw_src\n",
    "        \n",
    "        print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, question))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(question)\n",
    "        k.append(raw_src.split())\n",
    "    # calculate BLEU score\n",
    "    #print(actual)\n",
    "    #print('k=',k)\n",
    "    \n",
    "    return (actual,k,predicted)\n",
    "    \n",
    "def evaluate_model2(model, tokenizer, sources):\n",
    "    #actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        \n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        question = predict_sequence(model, tokenizer, source)\n",
    "        question = question.split()\n",
    "        question = ' '.join(question[:3])+' '+keywords[i]\n",
    "        \n",
    "        print('predicted question=[%s]  answer=[%s]' % (question+'?',answers[i]))\n",
    "        \n",
    "\n",
    "    # load datasets\n",
    "'''dataset = load_clean_sentences('k2q-both.pkl')\n",
    "train = load_clean_sentences('k2q-train.pkl')\n",
    "test = load_clean_sentences('k2q-test.pkl')'''\n",
    "# prepare question tokenizer\n",
    "'''q_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "q_vocab_size = len(q_tokenizer.word_index) + 1\n",
    "q_length = max_length(dataset[:, 0])\n",
    "# prepare keyword tokenizer\n",
    "k_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "k_vocab_size = len(k_tokenizer.word_index) + 1\n",
    "k_length = max_length(dataset[:, 1])'''\n",
    "# prepare data\n",
    "trainX = encode_sequences(k_tokenizer, k_length, test[:, 1])\n",
    "testX = encode_sequences(k_tokenizer, k_length, keywords)\n",
    "\n",
    "#trainX.reshape((2100,13,1))\n",
    "#trainX=array(trainX,dtype=float)\n",
    "#testX.reshape((105,13,1))\n",
    "#testX=array(trainX,dtype=float)\n",
    " \n",
    "# load model\n",
    "model = load_model('modelcopy2.h5')\n",
    "# test on some training sequences\n",
    "#print('train')\n",
    "#evaluate_model2(model, q_tokenizer, trainX)\n",
    "# test on some test sequences\n",
    "print('test:')\n",
    "actual,predicted,k=evaluate_model(model, q_tokenizer, trainX,test)\n",
    "\n",
    "print('\\nQuestion Answer pairs:')\n",
    "evaluate_model2(model, q_tokenizer, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual1=[]\n",
    "for i in actual:\n",
    "    actual1.append(' '.join(i))\n",
    "#print(actual1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted1=[]\n",
    "for i in predicted:\n",
    "    predicted1.append(' '.join(i))\n",
    "#print(predicted1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5853024666348912\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sum1=0\n",
    "for i in range(len(actual)):\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([predicted1[i]], actual1[i])\n",
    "    #rint(BLEUscore)\n",
    "    sum1=sum1+BLEUscore\n",
    "print(sum1/len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
